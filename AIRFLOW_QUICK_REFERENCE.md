# üéØ Resumen Ejecutivo: Airflow + Data Lakehouse

## ¬øQU√â ES AIRFLOW?

Imagina que tienes una f√°brica de datos. **Airflow es el gerente de la f√°brica** que:
- ‚è∞ Supervisa que todo ocurra en el horario correcto
- üîÑ Reinicia autom√°ticamente si algo falla
- üìä Te muestra reportes de lo que sucedi√≥
- üö® Te alerta si hay problemas

## ¬øCU√ÅNDO EJECUTA EL PIPELINE?

```
Todos los D√çAS a las 2:00 AM UTC (10 PM EDT)
  ‚Üì
  Extrae datos del API de OpenWeather
  ‚Üì
  Limpia y normaliza los datos
  ‚Üì
  Crea tablas anal√≠ticas con agregados
  ‚Üì
  Genera reporte JSON
  ‚Üì
  ‚úì Completado (o ‚úó Fallido ‚Üí Reintentos autom√°ticos)
```

## ¬øD√ìNDE GUARDA LOS DATOS?

### En el CONTENEDOR DOCKER (no en tu PC):

```
Contenedor: data-lakehouse-airflow
‚îú‚îÄ‚îÄ /app/data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/api/              ‚Üê Respuestas JSON del API (JSON crudos)
‚îÇ   ‚îú‚îÄ‚îÄ raw/csv/              ‚Üê CSVs importados (sin procesar)
‚îÇ   ‚îú‚îÄ‚îÄ clean/                ‚Üê Datos limpios (Parquet)
‚îÇ   ‚îî‚îÄ‚îÄ analytics/            ‚Üê Base de datos DuckDB (vistas SQL)
‚îî‚îÄ‚îÄ /app/outputs/
    ‚îî‚îÄ‚îÄ analytics_report.json ‚Üê Reporte final (JSON)
```

### En tu PC (sincroniazado):

```
/home/george/data-lakehouse-simulation/
‚îú‚îÄ‚îÄ data/                     ‚Üê Se sincroniza autom√°ticamente
‚îú‚îÄ‚îÄ outputs/                  ‚Üê Se sincroniza autom√°ticamente
‚îî‚îÄ‚îÄ logs/                     ‚Üê Logs de Airflow
```

## DATOS REALES DE EJEMPLO

### Entrada (API):
```json
{
  "city": "London",
  "date": "2025-12-22",
  "temperature_celsius": 5.2,
  "humidity_percent": 78,
  "wind_speed_kmh": 12.5
}
```

### Salida (Analytics Report):
```json
{
  "timestamp": "2025-12-22T14:00:00Z",
  "total_records": 150,
  "cities": ["London", "Tokyo", "New York", "Los Angeles", "Sydney"],
  "temperature_stats": {
    "average": 18.5,
    "max": 35.2,
    "min": -2.1
  },
  "execution_time_seconds": 45,
  "status": "SUCCESS"
}
```

## 3 FORMAS DE EJECUTAR

### 1Ô∏è‚É£ AUTOM√ÅTICAMEN (Recomendado)
- Airflow ejecuta autom√°ticamente cada d√≠a a las 2 AM UTC
- Todo ocurre en background (Docker)
- Monitoreas desde Web UI: http://localhost:8081

### 2Ô∏è‚É£ MANUAL VIA WEB UI
1. Abre http://localhost:8081
2. Busca `data_lakehouse_pipeline`
3. Clickea el bot√≥n de play (‚ñ∂)
4. Ve en vivo c√≥mo se ejecuta

### 3Ô∏è‚É£ MANUAL VIA TERMINAL
```bash
docker exec data-lakehouse-airflow \
  airflow dags trigger data_lakehouse_pipeline
```

## MONITOREO EN TIEMPO REAL

### Web UI (http://localhost:8081)
- **Dashboard**: Ve estado de todos los DAGs
- **DAGs**: Visualiza el flujo gr√°fico
- **Grid**: Historial de todas las ejecuciones
- **Logs**: Lee logs detallados de cada tarea

### Terminal (Logs en vivo)
```bash
docker logs -f data-lakehouse-airflow 2>&1 | grep -E "STAGE|‚úì|‚úó"
```

## LOGS ESPERADOS (EJEMPLO)

```
2025-12-22 14:00:00 - ================================================
2025-12-22 14:00:00 - STAGE 1: DATA INGESTION
2025-12-22 14:00:00 - ================================================
2025-12-22 14:00:01 - [1.1] Fetching data from OpenWeather API...
2025-12-22 14:00:02 - ‚úì API Request: London - Status 200 - 150 records
2025-12-22 14:00:03 - ‚úì Saved: /data/raw/api/london_2025-12-22.json
2025-12-22 14:00:04 - ‚úì API Request: Tokyo - Status 200 - 155 records
2025-12-22 14:00:05 - ‚úì Saved: /data/raw/api/tokyo_2025-12-22.json
2025-12-22 14:00:10 - ‚úì Successfully ingested 5 API files (750 total records)
2025-12-22 14:00:11 - ================================================
2025-12-22 14:00:11 - STAGE 2: TRANSFORMATION
2025-12-22 14:00:11 - ================================================
2025-12-22 14:00:12 - [2.1] Loading raw data...
2025-12-22 14:00:13 - Loaded 5 JSON files from API (750 records)
2025-12-22 14:00:14 - [2.2] Normalizing and cleaning data...
2025-12-22 14:00:15 - ‚úì Removed 0 duplicate records
2025-12-22 14:00:16 - ‚úì Handled missing values
2025-12-22 14:00:17 - ‚úì Standardized datetime formats
2025-12-22 14:00:18 - [2.3] Saving cleaned data...
2025-12-22 14:00:20 - ‚úì Saved: /data/clean/2025-12-22_merged_clean.parquet
2025-12-22 14:00:21 - ================================================
2025-12-22 14:00:21 - STAGE 3: ANALYTICS
2025-12-22 14:00:21 - ================================================
2025-12-22 14:00:22 - [3.1] Loading clean data...
2025-12-22 14:00:23 - Loaded 750 records from parquet
2025-12-22 14:00:24 - [3.2] Creating analytics views...
2025-12-22 14:00:25 - ‚úì Created VIEW: temperature_stats
2025-12-22 14:00:26 - ‚úì Created VIEW: city_weather_summary
2025-12-22 14:00:27 - ‚úì Created VIEW: daily_aggregates
2025-12-22 14:00:28 - [3.3] Generating report...
2025-12-22 14:00:30 - ‚úì Saved: /outputs/analytics_report.json
2025-12-22 14:00:31 - ================================================
2025-12-22 14:00:31 - HEALTH CHECK
2025-12-22 14:00:31 - ================================================
2025-12-22 14:00:32 - ‚úì Analytics report found and validated
2025-12-22 14:00:33 - ‚úì PIPELINE EXECUTION SUCCESSFUL
2025-12-22 14:00:33 - Total execution time: 33 seconds
```

## ESTRUCTURA DE DATOS

```
Raw Layer (JSON API)
    ‚Üì (150 registros √ó 5 ciudades = 750 filas)
    
    london_2025-12-22.json
    tokyo_2025-12-22.json
    new_york_2025-12-22.json
    los_angeles_2025-12-22.json
    sydney_2025-12-22.json

            ‚Üì‚Üì‚Üì TRANSFORMATION ‚Üì‚Üì‚Üì

Clean Layer (Parquet)
    ‚Üì (750 registros, limpios y normalizados)
    
    2025-12-22_merged_clean.parquet
    - Sin duplicados
    - Fechas normalizadas
    - Valores validados

            ‚Üì‚Üì‚Üì ANALYTICS ‚Üì‚Üì‚Üì

Analytics Layer (DuckDB)
    ‚Üì (Vistas SQL agregadas)
    
    temperature_stats:
    - city: London, avg_temp: 5.2, max: 12.1, min: -2.0
    - city: Tokyo, avg_temp: 8.5, max: 15.3, min: 2.1
    ... etc
    
    city_weather_summary:
    - city: London, total_records: 150, avg_humidity: 78%
    - city: Tokyo, total_records: 150, avg_humidity: 65%
    ... etc

            ‚Üì‚Üì‚Üì FINAL REPORT ‚Üì‚Üì‚Üì

Output (JSON)
    ‚Üì
    analytics_report.json
    {
      "timestamp": "2025-12-22T14:00:00Z",
      "total_records": 750,
      "cities": 5,
      "temperature_avg": 18.5,
      "execution_time_seconds": 33,
      "status": "SUCCESS"
    }
```

## PR√ìXIMAS EJECUCIONES PROGRAMADAS

| DAG | Pr√≥xima Ejecuci√≥n | Frecuencia |
|-----|------------------|-----------|
| `data_lakehouse_pipeline` | Ma√±ana 2:00 AM UTC | Diariamente |
| `data_generation_pipeline` | Este domingo | Semanalmente |

---

## üîó REFERENCIAS R√ÅPIDAS

- **Web UI**: http://localhost:8081 (admin/admin)
- **Documentaci√≥n detallada**: `AIRFLOW_EXECUTION_GUIDE.md`
- **Ver logs en vivo**: `docker logs -f data-lakehouse-airflow`
- **Ejecutar manualmente**: `docker exec data-lakehouse-airflow airflow dags trigger data_lakehouse_pipeline`

---
