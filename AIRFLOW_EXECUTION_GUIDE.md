# ğŸš€ GuÃ­a de EjecuciÃ³n y Logging del Pipeline Airflow

## Â¿QuÃ© hace Airflow en este proyecto?

**Apache Airflow** es un orquestador de workflows que automatiza la ejecuciÃ³n del pipeline de datos en horarios programados.

### Funciones principales:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              APACHE AIRFLOW SCHEDULER                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â° SCHEDULER (Ejecuta en background)                       â”‚
â”‚   â””â”€ Monitorea los DAGs cada 5 minutos                      â”‚
â”‚   â””â”€ Ejecuta tareas segÃºn el cronograma configurado         â”‚
â”‚   â””â”€ Reintentar automÃ¡ticamente si falla (2 intentos)      â”‚
â”‚   â””â”€ Registra logs detallados de cada ejecuciÃ³n             â”‚
â”‚                                                              â”‚
â”‚  ğŸŒ WEB UI (Puerto 8081)                                    â”‚
â”‚   â””â”€ Visualiza DAGs disponibles                             â”‚
â”‚   â””â”€ Monitorea ejecuciones en tiempo real                   â”‚
â”‚   â””â”€ Ve logs de cada tarea                                  â”‚
â”‚   â””â”€ Ejecuta DAGs manualmente si lo deseas                  â”‚
â”‚                                                              â”‚
â”‚  ğŸ“Š DATABASE (SQLite)                                       â”‚
â”‚   â””â”€ Almacena metadata de ejecuciones                       â”‚
â”‚   â””â”€ Registra estado de tareas (success/failed)             â”‚
â”‚   â””â”€ Guarda histÃ³rico de ejecutios                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“… Cronograma de Ejecuciones

### DAG: `data_lakehouse_pipeline` (Principal)
- **Horario**: Diariamente a las **2:00 AM UTC** (10:00 PM EDT)
- **Frecuencia**: Todos los dÃ­as
- **DuraciÃ³n estimada**: 30-60 segundos
- **Reintentos**: 2 intentos con 5 minutos de espera

**Estructura de tareas**:
```
ingestion_stage
    â†“
transformation_stage
    â†“
analytics_stage
    â†“
health_check (verifica que todo estÃ¡ OK)
```

### DAG: `data_generation_pipeline` (Generador de datos)
- **Horario**: Domingos a **00:00 UTC** (8:00 PM sÃ¡bado EDT)
- **Frecuencia**: Una vez por semana
- **DuraciÃ³n estimada**: 5-10 segundos
- **PropÃ³sito**: Generar datos de prueba frescos

---

## ğŸ“‚ UbicaciÃ³n de Datos ExtraÃ­dos

### Estructura de directorios:

```
/home/george/data-lakehouse-simulation/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # ğŸ”´ CAPA RAW (Datos sin procesar)
â”‚   â”‚   â”œâ”€â”€ api/                      
â”‚   â”‚   â”‚   â”œâ”€â”€ london_2025-12-22.json    # API OpenWeather JSON
â”‚   â”‚   â”‚   â”œâ”€â”€ tokyo_2025-12-22.json
â”‚   â”‚   â”‚   â”œâ”€â”€ new_york_2025-12-22.json
â”‚   â”‚   â”‚   â”œâ”€â”€ los_angeles_2025-12-22.json
â”‚   â”‚   â”‚   â””â”€â”€ sydney_2025-12-22.json
â”‚   â”‚   â””â”€â”€ csv/
â”‚   â”‚       â””â”€â”€ sample_historical.csv     # Datos histÃ³ricos CSV
â”‚   â”‚
â”‚   â”œâ”€â”€ clean/                        # ğŸŸ¢ CAPA CLEAN (Datos normalizados)
â”‚   â”‚   â””â”€â”€ 2025-12-22_merged_clean.parquet
â”‚   â”‚
â”‚   â””â”€â”€ analytics/                    # ğŸ”µ CAPA ANALYTICS (Agregados)
â”‚       â”œâ”€â”€ analytics.db              # DuckDB con vistas analÃ­ticas
â”‚       â””â”€â”€ reports/
â”‚           â””â”€â”€ analytics_report.json # Reporte JSON final
â”‚
â”œâ”€â”€ outputs/                          # Resultados finales
â”‚   â””â”€â”€ analytics_report.json         # Mismo que arriba (copia)
â”‚
â”œâ”€â”€ logs/                             # Logs de Airflow
â”‚   â””â”€â”€ data_lakehouse_pipeline/
â”‚       â”œâ”€â”€ ingestion_stage/
â”‚       â”œâ”€â”€ transformation_stage/
â”‚       â”œâ”€â”€ analytics_stage/
â”‚       â””â”€â”€ health_check/
â”‚
â””â”€â”€ airflow_home/
    â”œâ”€â”€ airflow.db                    # Database SQLite de Airflow
    â””â”€â”€ logs/                         # Logs de ejecuciÃ³n
```

---

## ğŸ” Proceso Detallado de ExtracciÃ³n (Con Logs)

### **STAGE 1: INGESTION** (ExtracciÃ³n)

Airflow ejecuta:
```python
python3 scripts/run_pipeline.py  # FunciÃ³n: run_ingestion_stage()
```

**Logs esperados**:
```
[2025-12-22 14:00:00] INFO - ===============================================
[2025-12-22 14:00:00] INFO - STAGE 1: DATA INGESTION
[2025-12-22 14:00:00] INFO - ===============================================
[2025-12-22 14:00:01] INFO - [1.1] Fetching data from OpenWeather API...
[2025-12-22 14:00:02] INFO - âœ“ API Request: London - Status 200
[2025-12-22 14:00:03] INFO - âœ“ Saved: /data/raw/api/london_2025-12-22.json
[2025-12-22 14:00:04] INFO - âœ“ API Request: Tokyo - Status 200
[2025-12-22 14:00:05] INFO - âœ“ Saved: /data/raw/api/tokyo_2025-12-22.json
...
[2025-12-22 14:00:15] INFO - âœ“ Successfully ingested 5 API files
[2025-12-22 14:00:16] INFO - [1.2] Ingesting CSV data...
[2025-12-22 14:00:17] INFO - âœ“ CSV ingestion successful
```

**Â¿QuÃ© sucede?**:
1. Conecta a OpenWeather API con tu clave
2. Obtiene datos de 5 ciudades (London, Tokyo, New York, Los Angeles, Sydney)
3. Guarda cada respuesta como JSON en `data/raw/api/`
4. Si existe CSV, lo copia a `data/raw/csv/`
5. Registra todo en logs

---

### **STAGE 2: TRANSFORMATION** (Limpieza)

Airflow ejecuta:
```python
python3 scripts/run_pipeline.py  # FunciÃ³n: run_transformation_stage()
```

**Logs esperados**:
```
[2025-12-22 14:00:20] INFO - ===============================================
[2025-12-22 14:00:20] INFO - STAGE 2: TRANSFORMATION
[2025-12-22 14:00:20] INFO - ===============================================
[2025-12-22 14:00:21] INFO - [2.1] Loading raw data...
[2025-12-22 14:00:22] INFO - Loaded 5 JSON files from API
[2025-12-22 14:00:23] INFO - [2.2] Normalizing and cleaning data...
[2025-12-22 14:00:24] INFO - âœ“ Removed 0 duplicate records
[2025-12-22 14:00:25] INFO - âœ“ Handled missing values
[2025-12-22 14:00:26] INFO - âœ“ Standardized datetime formats
[2025-12-22 14:00:27] INFO - [2.3] Saving cleaned data...
[2025-12-22 14:00:28] INFO - âœ“ Saved: /data/clean/2025-12-22_merged_clean.parquet
```

**Â¿QuÃ© sucede?**:
1. Lee JSONs del API desde `data/raw/api/`
2. Lee CSV si existe desde `data/raw/csv/`
3. Normaliza formatos de fecha/hora
4. Valida datos numÃ©ricos (temperatura, humedad, etc.)
5. Elimina duplicados
6. Guarda como Parquet en `data/clean/`

---

### **STAGE 3: ANALYTICS** (Agregados)

Airflow ejecuta:
```python
python3 scripts/run_pipeline.py  # FunciÃ³n: run_analytics_stage()
```

**Logs esperados**:
```
[2025-12-22 14:00:30] INFO - ===============================================
[2025-12-22 14:00:30] INFO - STAGE 3: ANALYTICS
[2025-12-22 14:00:30] INFO - ===============================================
[2025-12-22 14:00:31] INFO - [3.1] Loading clean data...
[2025-12-22 14:00:32] INFO - Loaded 150 records
[2025-12-22 14:00:33] INFO - [3.2] Creating analytics views...
[2025-12-22 14:00:34] INFO - âœ“ Created VIEW: temperature_stats
[2025-12-22 14:00:35] INFO - âœ“ Created VIEW: city_weather_summary
[2025-12-22 14:00:36] INFO - âœ“ Created VIEW: daily_aggregates
[2025-12-22 14:00:37] INFO - [3.3] Generating report...
[2025-12-22 14:00:38] INFO - âœ“ Saved: /outputs/analytics_report.json
```

**Â¿QuÃ© sucede?**:
1. Lee datos limpios desde `data/clean/`
2. Crea base de datos DuckDB en `data/analytics/analytics.db`
3. Crea 3 vistas SQL agregadas:
   - `temperature_stats`: Promedio, mÃ¡ximo, mÃ­nimo por ciudad
   - `city_weather_summary`: Resumen general por ciudad
   - `daily_aggregates`: Totales diarios
4. Ejecuta queries de anÃ¡lisis
5. Genera reporte JSON en `outputs/analytics_report.json`

---

### **STAGE 4: HEALTH CHECK**

Airflow ejecuta:
```bash
test -f outputs/analytics_report.json && echo "âœ“ Pipeline successful"
```

**Logs esperados**:
```
[2025-12-22 14:00:40] INFO - ===============================================
[2025-12-22 14:00:40] INFO - HEALTH CHECK
[2025-12-22 14:00:40] INFO - ===============================================
[2025-12-22 14:00:41] INFO - âœ“ Analytics report found
[2025-12-22 14:00:42] INFO - âœ“ Pipeline execution successful
```

---

## ğŸ“Š Ver Logs en Airflow Web UI

### **En el navegador (http://localhost:8081)**:

1. **Ir a DAGs** â†’ `data_lakehouse_pipeline`
2. **Ver grÃ¡fico** â†’ Visualiza las 4 tareas
3. **Ir a "Grid"** â†’ Ve todas las ejecuciones
4. **Clickear una ejecuciÃ³n** â†’ Ve detalles
5. **Clickear una tarea** â†’ **Log** â†’ Lee logs completos

### **En la terminal** (dentro del container):

```bash
docker exec data-lakehouse-airflow bash -c \
  "tail -100f /airflow/logs/data_lakehouse_pipeline/*/latest/log"
```

---

## ğŸ”§ EjecuciÃ³n Manual del Pipeline

Si quieres ejecutar ahora sin esperar:

```bash
# OpciÃ³n 1: Via Airflow CLI
docker exec data-lakehouse-airflow airflow dags trigger data_lakehouse_pipeline

# OpciÃ³n 2: Via UI
# 1. Ve a http://localhost:8081
# 2. En "DAGs", busca data_lakehouse_pipeline
# 3. Clickea el botÃ³n de play (â–¶)
```

---

## ğŸ“ˆ PrÃ³xima EjecuciÃ³n

- **data_lakehouse_pipeline**: MaÃ±ana a las 2:00 AM UTC
- **data_generation_pipeline**: Este domingo a medianoche UTC

Puedes ver las prÃ³ximas ejecuciones en la columna "Next Run" en la Web UI.

---

## ğŸ¯ Resumen de Flujo de Datos

```
OpenWeather API        CSV (opcional)
       â†“                    â†“
  [INGESTION]       ----- Extrae datos crudos
       â†“
  /data/raw/
       â†“
  [TRANSFORMATION]   ----- Limpia y normaliza
       â†“
  /data/clean/
       â†“
  [ANALYTICS]        ----- Crea agregados
       â†“
  /data/analytics/
  + /outputs/
       â†“
  ğŸ“Š analytics_report.json  ----- Resultado final
```

---

## â“ Preguntas Frecuentes

**P: Â¿DÃ³nde veo si fallÃ³ algo?**
- R: Web UI â†’ DAG â†’ Tarea roja â†’ Log

**P: Â¿CÃ³mo evito que se ejecute automÃ¡ticamente?**
- R: En la UI, pausa el DAG (toggle ON/OFF)

**P: Â¿CÃ³mo cambio el horario?**
- R: Edita `dags/main_pipeline_dag.py` â†’ `schedule_interval`

**P: Â¿Se sobrescribe la data cada dÃ­a?**
- R: SÃ­, cada dÃ­a sobrescribe `data/clean/` y `outputs/` con datos nuevos

---
