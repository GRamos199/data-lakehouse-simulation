# Data Lakehouse Architecture Simulation

A complete local data lakehouse implementation with three data layers: raw, clean, and analytics. Ingests weather data from OpenWeather API and CSV files, then transforms them through a production-grade pipeline.

## ğŸš€ Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Configuration

Create a `.env` file in the project root:

```env
OPENWEATHER_API_KEY=your_api_key_here
```

Or edit `.env` directly with your OpenWeather API key.

### Run the Pipeline

```bash
python3 scripts/run_pipeline.py
```

This will:
1. **Ingest** - Fetch weather data from OpenWeather API + CSV files
2. **Transform** - Normalize and flatten data
3. **Analyze** - Create DuckDB tables and run analytics

## ğŸ“Š Architecture

**Three-Layer Lakehouse:**
- **Raw (Bronze)** - Original untransformed data (JSON, CSV)
- **Clean (Silver)** - Normalized, deduplicated data
- **Analytics (Gold)** - Optimized DuckDB tables and SQL views

**Data Flow:**
```
API Data + CSV Files â†’ Raw Layer â†’ Clean Layer â†’ DuckDB Database â†’ Analytics Report
```

## ğŸ“ Project Structure

```
data-lakehouse-simulation/
â”œâ”€â”€ config/config.py                    # Configuration management
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ api_ingestion.py           # OpenWeather API ingestion
â”‚   â”‚   â””â”€â”€ csv_ingestion.py           # CSV file ingestion
â”‚   â”œâ”€â”€ transformations/
â”‚   â”‚   â”œâ”€â”€ raw_to_clean.py            # Flatten & normalize
â”‚   â”‚   â””â”€â”€ clean_to_analytics.py      # Create DuckDB tables
â”‚   â””â”€â”€ analytics/
â”‚       â””â”€â”€ queries.py                  # SQL queries & reports
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py                # Main orchestration
â”‚   â””â”€â”€ generate_sample_data.py        # Generate test CSV
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                           # Original data
â”‚   â”œâ”€â”€ clean/                         # Normalized data
â”‚   â””â”€â”€ analytics/                     # DuckDB database + reports
â”œâ”€â”€ .env                               # API key (git ignored)
â””â”€â”€ requirements.txt                   # Python dependencies
```

## ğŸ”§ Technologies

- **Python 3.8+** - Core processing language
- **OpenWeather API** - Weather data source
- **Pandas** - Data processing
- **DuckDB** - Analytical database (local, serverless)
- **Python-dotenv** - Environment variable management

## ğŸ“ˆ Output

After running the pipeline:

- **Raw Data** - `data/raw/api/*.json` and `data/raw/csv/*.csv`
- **Clean Data** - `data/clean/*.csv`
- **Database** - `data/analytics/lakehouse.duckdb`
- **Report** - `data/analytics/analytics_report.json`

The analytics report includes:
- Overall weather statistics
- Latest temperatures by city
- Temperature trends over time
- Weather condition distribution
- Extreme weather events

## ğŸ¯ Features

âœ… Real-time API data ingestion with error handling
âœ… CSV file processing with metadata tracking
âœ… Data validation and quality checks
âœ… Automatic table creation in DuckDB
âœ… Multiple analytical views (daily summary, city comparison)
âœ… Comprehensive JSON report generation
âœ… Fully typed Python code with docstrings
âœ… Production-ready error handling and logging

## ğŸ’¡ Usage Examples

```python
from src.analytics.queries import AnalyticsQueryEngine

# Initialize the analytics engine
engine = AnalyticsQueryEngine()

# Get weather summary
summary = engine.get_weather_summary()

# Get temperatures by city
cities = engine.get_city_temperatures()

# Generate full report
report = engine.generate_analytics_report()
```

## ğŸ” Security

API keys are stored in `.env` and git-ignored. Never commit credentials to version control.

## ğŸ“ License

Open source project for educational and portfolio purposes.
