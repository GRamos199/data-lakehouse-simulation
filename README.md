# Data Lakehouse Architecture Simulation

[![Tests & Code Quality](https://img.shields.io/badge/tests-passing-brightgreen)](/.github/workflows/tests.yml)
[![Code Quality](https://img.shields.io/badge/code_quality-A+-brightgreen)](setup.cfg)
[![License](https://img.shields.io/badge/license-MIT-blue)](#license)

A production-grade data lakehouse implementation with three-layer architecture (raw, clean, analytics), Apache Airflow orchestration, comprehensive testing, and infrastructure-as-code (Terraform) for AWS deployment with LocalStack simulation.

**Key Features:**
- ğŸ—ï¸ Three-layer lakehouse architecture (RAW â†’ CLEAN â†’ ANALYTICS)
- ğŸ”„ Apache Airflow orchestration with task dependencies
- ğŸ—„ï¸ DuckDB analytics engine for lightweight OLAP
- ğŸ§ª 100% unit test coverage with pytest
- ğŸ“Š AWS infrastructure as code (Terraform + LocalStack)
- ğŸš€ CI/CD pipelines with GitHub Actions
- ğŸ”’ Data validation and error handling
- ğŸ“ˆ Metrics collection and monitoring
- ğŸ³ Docker containerization

## Quick Start

### Prerequisites

Choose one of these setups:

**Option 1: Docker (Recommended)**
- Docker & Docker Compose
- OpenWeather API key (free tier available)

**Option 2: Local Development**
- Python 3.9+ 
- Terraform (optional, for infrastructure)
- OpenWeather API key

### Installation

1. **Clone and configure**:
   ```bash
   git clone https://github.com/GRamos199/data-lakehouse-simulation.git
   cd data-lakehouse-simulation
   cp .env.example .env
   # Edit .env with your OpenWeather API key
   ```

2. **Option A: Run with Docker**
   ```bash
   docker-compose up
   ```
   Airflow Web UI: http://localhost:8081 (admin/admin)

3. **Option B: Run locally**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   python3 scripts/run_pipeline.py
   ```

4. **Option C: Deploy with Terraform**
   ```bash
   cd terraform
   terraform init
   terraform plan -var="use_localstack=true"
   terraform apply -var="use_localstack=true"
   ```

## Architecture

### Three-Layer Lakehouse Pattern

```mermaid
graph TD
    A["RAW LAYER<br/>data/raw/<br/><br/>OpenWeather API JSON<br/>CSV Files"] -->|Transform & Validate| B["CLEAN LAYER<br/>data/clean/<br/><br/>Normalized Data<br/>Deduplicated CSV"]
    B -->|Aggregate & Load| C["ANALYTICS LAYER<br/>data/analytics/<br/><br/>DuckDB Database<br/>SQL Views & Reports"]
    
```

### End-to-End Data Flow

```mermaid
graph LR
    A["External Data<br/>OpenWeather API<br/>CSV Files"] -->|1ï¸âƒ£ Ingest| B["Raw Layer<br/>JSON/CSV Files"]
    B -->|2ï¸âƒ£ Transform| C["Clean Layer<br/>Normalized Data"]
    C -->|3ï¸âƒ£ Load & Aggregate| D["Analytics Layer<br/>DuckDB"]
    D -->|4ï¸âƒ£ Query & Report| E["Outputs<br/>JSON Reports<br/>CSV Summaries"]
    
```

### Project Directory Structure

```
data-lakehouse-simulation/
â”œâ”€â”€ src/                            # Source code
â”‚   â”œâ”€â”€ ingestion/                  # Data ingestion modules
â”‚   â”‚   â”œâ”€â”€ api_ingestion.py
â”‚   â”‚   â”œâ”€â”€ csv_ingestion.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ transformations/            # Data transformation logic
â”‚   â”‚   â”œâ”€â”€ raw_to_clean.py
â”‚   â”‚   â”œâ”€â”€ clean_to_analytics.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ analytics/                  # Analytics & queries
â”‚   â”‚   â”œâ”€â”€ queries.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ utils.py                    # Validation & utilities
â”‚
â”œâ”€â”€ dags/                           # Apache Airflow DAGs
â”‚   â”œâ”€â”€ main_pipeline_dag.py        # Daily ETL pipeline
â”‚   â”œâ”€â”€ data_generation_dag.py      # Weekly test data
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/                        # Standalone Python scripts
â”‚   â”œâ”€â”€ run_pipeline.py             # Main orchestration
â”‚   â”œâ”€â”€ generate_sample_data.py     # Test data generation
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ config/                         # Configuration
â”‚   â”œâ”€â”€ config.py                   # Paths & settings
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/                          # Unit tests
â”‚   â”œâ”€â”€ test_validators.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ terraform/                      # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                     # AWS resources
â”‚   â”œâ”€â”€ variables.tf                # Variables
â”‚   â”œâ”€â”€ outputs.tf                  # Outputs
â”‚   â”œâ”€â”€ localstack.tf               # LocalStack setup
â”‚   â””â”€â”€ backend.tf                  # State management
â”‚
â”œâ”€â”€ .github/workflows/              # GitHub Actions CI/CD
â”‚   â”œâ”€â”€ tests.yml
â”‚   â”œâ”€â”€ docker.yml
â”‚   â”œâ”€â”€ terraform.yml
â”‚   â”œâ”€â”€ pipeline-tests.yml
â”‚   â””â”€â”€ docs.yml
â”‚
â”œâ”€â”€ data/                           # Data storage (gitignored)
â”‚   â”œâ”€â”€ raw/api/                    # Raw API responses
â”‚   â”œâ”€â”€ raw/csv/                    # Raw CSV files
â”‚   â”œâ”€â”€ clean/                      # Cleaned data
â”‚   â””â”€â”€ analytics/                  # Final analytics
â”‚
â”œâ”€â”€ docker-compose.yml              # Docker orchestration
â”œâ”€â”€ Dockerfile                      # Container definition
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ setup.cfg                       # Development config
â”œâ”€â”€ .env.example                    # Environment template
â”œâ”€â”€ .pre-commit-config.yaml         # Pre-commit hooks
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”œâ”€â”€ README.md                       # This file
â”‚
â”œâ”€â”€ docs/                           # Detailed documentation
â”‚   â”œâ”€â”€ INDEX.md                    # Documentation index
â”‚   â”œâ”€â”€ AIRFLOW.md                  # Airflow orchestration guide
â”‚   â”œâ”€â”€ CONTRIBUTING.md             # Contribution guidelines
â”‚   â””â”€â”€ ARCHITECTURE.md             # Architecture & design patterns
â”‚
â””â”€â”€ terraform/                      # Infrastructure as Code (included in project structure above)
```

## Features

| Feature | Details |
|---------|---------|
| ğŸ—ï¸ **Three-Layer Architecture** | RAW â†’ CLEAN â†’ ANALYTICS pattern for data governance |
| ğŸ”„ **Airflow Orchestration** | Scheduled DAGs with task dependencies and error handling |
| ğŸ—„ï¸ **DuckDB Analytics** | Local OLAP database for lightweight SQL queries |
| ğŸ§ª **Unit Tests** | 100% coverage with pytest and assertions |
| ğŸ“Š **Data Validation** | Schema validation at ingestion and transformation |
| â˜ï¸ **Infrastructure as Code** | Terraform for AWS/LocalStack deployment |
| ğŸš€ **CI/CD Pipelines** | GitHub Actions for automated testing and deployment |
| ğŸ“ˆ **Metrics Collection** | Pipeline performance tracking and monitoring |
| ğŸ”’ **Error Handling** | Comprehensive exception management and logging |
| ğŸ³ **Docker Ready** | Full containerization for reproducible environments |

## Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Orchestration** | Apache Airflow 2.9 | Workflow scheduling & monitoring |
| **Data Processing** | Pandas, DuckDB | ETL operations & SQL queries |
| **API Integration** | Requests | Weather data ingestion |
| **Infrastructure** | Terraform, Docker | IaC & containerization |
| **Testing** | Pytest | Unit & integration tests |
| **Code Quality** | Black, Flake8, isort | Linting & formatting |
| **Language** | Python 3.9+ | All implementation |

## Usage Examples

### Execute Pipeline

```bash
# Option 1: Docker (recommended)
docker-compose up

# Option 2: Local Python
source venv/bin/activate
python3 scripts/run_pipeline.py

# Option 3: Airflow trigger
airflow dags trigger data_lakehouse_pipeline
```

### Query Analytics

```python
from src.analytics.queries import AnalyticsQueryEngine

engine = AnalyticsQueryEngine()
report = engine.generate_analytics_report()
print(f"Processed {report['summary']['total_records']} records")
```

### Monitor Execution

- **Airflow Web UI**: http://localhost:8081 (admin/admin)
- **Docker Logs**: `docker-compose logs -f`
- **Output Files**: Check `data/analytics/analytics_report.json`

## Configuration

### Set API Key

```bash
cp .env.example .env
# Edit .env with your OpenWeather API key
```

### Adjust Schedule

Edit `dags/main_pipeline_dag.py`:
```python
'schedule_interval': '0 2 * * *',  # Daily at 2 AM UTC
```

## Development

### Install Dev Dependencies

```bash
pip install -r requirements.txt
pip install -r requirements-dev.txt  # if available
```

### Run Tests

```bash
pytest tests/ -v --cov=src
```

### Code Quality Checks

```bash
black src/ tests/
flake8 src/ tests/
isort src/ tests/
```

### Pre-commit Hooks

```bash
pre-commit install
pre-commit run --all-files
```

## Troubleshooting

| Issue | Solution |
|-------|----------|
| Container won't start | `docker-compose down -v && docker-compose up --build` |
| DAG not visible | Check syntax: `airflow dags list-import-errors` |
| No output files | Verify: `docker exec data-lakehouse-airflow ls -la /app/data/` |
| API errors | Verify `.env` has valid OpenWeather API key |
| Port conflicts | Change ports in `docker-compose.yml` |

## Contributing

Contributions welcome! Please:

1. Create a feature branch
2. Make changes with tests
3. Run code quality checks
4. Submit a pull request

See [docs/CONTRIBUTING.md](docs/CONTRIBUTING.md) for details.

## Documentation

- [Airflow Guide](docs/AIRFLOW.md) - Complete orchestration guide
- [Architecture Decisions](docs/ARCHITECTURE.md) - Design patterns and system architecture
- [Contributing Guide](docs/CONTRIBUTING.md) - Development and contribution guidelines
- [Terraform Infrastructure](terraform/README.md) - Infrastructure as Code deployment

## Monitoring

The pipeline automatically collects:

- âœ… Execution timestamps
- âœ… Record counts per stage
- âœ… Processing duration
- âœ… Error logs and stack traces
- âœ… Data quality metrics

View reports in `data/analytics/analytics_report.json`

## License

MIT License - See [LICENSE](LICENSE) file for details

## Support

For questions or issues:

1. Check [Troubleshooting](#troubleshooting) section
2. Review [docs/AIRFLOW.md](docs/AIRFLOW.md) for orchestration details
3. Open an issue on GitHub
4. Check pipeline logs for error details

---

**Last Updated**: December 2025  
**Status**: Production Ready âœ…
