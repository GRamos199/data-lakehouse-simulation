# Data Lakehouse Architecture Simulation

A complete local data lakehouse implementation with three data layers: raw, clean, and analytics. Ingests weather data from OpenWeather API and CSV files, then transforms them through a production-grade pipeline with Apache Airflow orchestration.

## ğŸš€ Quick Start

### Installation

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Configuration

Create a `.env` file in the project root:

```env
OPENWEATHER_API_KEY=your_api_key_here
```

### Run Once (Manual)

```bash
python3 scripts/run_pipeline.py
```

### Run with Apache Airflow (Recommended)

```bash
# Setup Airflow
bash scripts/setup_airflow.sh

# Terminal 1: Start webserver
airflow webserver -p 8080

# Terminal 2: Start scheduler
airflow scheduler

# Access: http://localhost:8080 (admin/admin)
```

## ğŸ“Š Architecture

**Three-Layer Lakehouse:**
- **Raw (Bronze)** - Original untransformed data (JSON, CSV)
- **Clean (Silver)** - Normalized, deduplicated data
- **Analytics (Gold)** - Optimized DuckDB tables and SQL views

**Data Flow:**
```
API Data + CSV Files â†’ Raw Layer â†’ Clean Layer â†’ DuckDB Database â†’ Analytics Report
```

## ğŸ“ Project Structure

```
data-lakehouse-simulation/
â”œâ”€â”€ config/config.py                    # Configuration management
â”œâ”€â”€ dags/                               # Airflow DAGs (scheduled pipelines)
â”‚   â”œâ”€â”€ main_pipeline_dag.py           # Daily ETL orchestration (2:00 AM UTC)
â”‚   â””â”€â”€ data_generation_dag.py         # Weekly sample data generation

â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ api_ingestion.py           # OpenWeather API ingestion
â”‚   â”‚   â””â”€â”€ csv_ingestion.py           # CSV file ingestion
â”‚   â”œâ”€â”€ transformations/
â”‚   â”‚   â”œâ”€â”€ raw_to_clean.py            # Flatten & normalize
â”‚   â”‚   â””â”€â”€ clean_to_analytics.py      # Create DuckDB tables
â”‚   â””â”€â”€ analytics/
â”‚       â””â”€â”€ queries.py                  # SQL queries & reports
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py                # Main orchestration
â”‚   â”œâ”€â”€ generate_sample_data.py        # Generate test CSV
â”‚   â””â”€â”€ setup_airflow.sh               # Airflow initialization
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                           # Original data
â”‚   â”œâ”€â”€ clean/                         # Normalized data
â”‚   â””â”€â”€ analytics/                     # DuckDB database + reports
â”œâ”€â”€ airflow_home/                      # Airflow working directory (created on setup)
â”œâ”€â”€ .env                               # API key (git ignored)
â”œâ”€â”€ airflow.cfg                        # Airflow configuration
â””â”€â”€ requirements.txt                   # Python dependencies
```

## ğŸ”§ Technologies

- **Python 3.8+** - Core processing language
- **Apache Airflow** - Workflow orchestration & scheduling
- **OpenWeather API** - Weather data source
- **Pandas** - Data processing
- **DuckDB** - Analytical database (local, serverless)

## â° Scheduling with Airflow

### Available DAGs:

| DAG | Schedule | Purpose |
|-----|----------|---------|
| `data_lakehouse_pipeline` | Daily @ 2:00 AM UTC | Complete ETL pipeline |
| `data_generation_pipeline` | Weekly (Sunday midnight) | Generate test data |

### Monitor Execution:

- **Webserver UI**: http://localhost:8080
- **Logs**: `airflow_home/logs/`
- **Database**: `airflow_home/airflow.db`

## ğŸ“ˆ Output

After running the pipeline:

- **Raw Data** - `data/raw/api/*.json` and `data/raw/csv/*.csv`
- **Clean Data** - `data/clean/*.csv`
- **Database** - `data/analytics/lakehouse.duckdb`
- **Report** - `data/analytics/analytics_report.json`
- **Analytics Views** - `data/analytics/*.csv` (exported)

The analytics report includes:
- Overall weather statistics (5 cities, 150 records)
- Latest temperatures by city
- Weather condition distribution
- Extreme weather events

## ğŸ¯ Features

âœ… Apache Airflow orchestration with task groups
âœ… Real-time API data ingestion with error handling
âœ… CSV file processing with metadata tracking
âœ… Data validation and quality checks
âœ… Automatic table creation in DuckDB
âœ… Multiple analytical views (daily summary, city comparison)
âœ… Comprehensive JSON report generation
âœ… Fully typed Python code with docstrings
âœ… Production-ready error handling and logging
âœ… Scheduled automation (no manual intervention needed)

## ğŸ’¡ Usage Examples

### Manual Execution:

```bash
python3 scripts/run_pipeline.py
```

### Airflow Execution:

```bash
# Trigger specific DAG
airflow dags trigger data_lakehouse_pipeline

# Check DAG status
airflow dags list
airflow tasks list data_lakehouse_pipeline
```

### Query Analytics:

```python
from src.analytics.queries import AnalyticsQueryEngine

engine = AnalyticsQueryEngine()
summary = engine.get_weather_summary()
print(summary)
```

## ğŸ” Security

- API keys stored in `.env` file (git-ignored)
- Airflow secret key in `airflow.cfg` (change in production)
- No credentials in source code
- Environment-based configuration

## ğŸ“ License

Open source project for educational and portfolio purposes.

cities = engine.get_city_temperatures()

# Generate full report
report = engine.generate_analytics_report()
```

## ğŸ” Security

API keys are stored in `.env` and git-ignored. Never commit credentials to version control.

## ğŸ“ License

Open source project for educational and portfolio purposes.
