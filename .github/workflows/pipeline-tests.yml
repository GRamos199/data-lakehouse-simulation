name: Pipeline Integration Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run every day at 2 AM UTC (same as pipeline schedule)
    - cron: '0 2 * * *'

jobs:
  integration-tests:
    runs-on: ubuntu-latest
    environment: prod
    
    services:
      localstack:
        image: localstack/localstack:latest
        ports:
          - 4566:4566
        env:
          SERVICES: s3,dynamodb,sns,sqs,cloudwatch,logs,events
          DEBUG: 1
        options: >-
          --health-cmd "curl -f http://localhost:4566/_localstack/health || exit 1"
          --health-interval 5s
          --health-timeout 2s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-integration
      
      - name: Create test data
        run: |
          python scripts/generate_sample_data.py
      
      - name: Run ingestion stage
        env:
          OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
          LOCALSTACK_ENDPOINT_URL: http://localhost:4566
        run: |
          python -c "
          from scripts.run_pipeline import DataLakehousePipeline
          pipeline = DataLakehousePipeline()
          success = pipeline.run_ingestion_stage()
          print('✓ Ingestion stage completed')
          "
      
      - name: Run transformation stage
        run: |
          python -c "
          from scripts.run_pipeline import DataLakehousePipeline
          pipeline = DataLakehousePipeline()
          success = pipeline.run_transformation_stage()
          print('✓ Transformation stage completed')
          "
      
      - name: Run analytics stage
        run: |
          python -c "
          from scripts.run_pipeline import DataLakehousePipeline
          pipeline = DataLakehousePipeline()
          success = pipeline.run_analytics_stage()
          print('✓ Analytics stage completed')
          "
      
      - name: Verify output
        run: |
          python -c "
          import json
          from pathlib import Path
          
          # Check if analytics report was created
          report_path = Path('data/analytics/analytics_report.json')
          if report_path.exists():
              with open(report_path) as f:
                  report = json.load(f)
              print('✓ Analytics report generated successfully')
              print(f'  Cities: {report.get(\"summary\", {}).get(\"cities_loaded\", [])}')
          else:
              print('✗ Analytics report not found')
              exit(1)
          "
      
      - name: Upload pipeline outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-outputs
          path: data/analytics/
      
      - name: Generate report
        if: always()
        run: |
          python -c "
          import json
          from pathlib import Path
          
          # Create summary report
          report_path = Path('data/analytics/analytics_report.json')
          if report_path.exists():
              with open(report_path) as f:
                  data = json.load(f)
              print('## Pipeline Execution Summary')
              print()
              print('✓ Pipeline completed successfully')
          else:
              print('## Pipeline Execution Failed')
          "
